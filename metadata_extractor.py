"""
Metadata Extractor Plugin - Extract metadata from various file formats
"""

import struct
import imghdr
from pathlib import Path
from typing import Dict, List, Optional, Any, Union, Tuple
from rage_plugins.base_plugin import RAGEPlugin 
import AnalysisPlugin


class MetadataExtractorPlugin(AnalysisPlugin):
    """Metadata extraction from various file formats"""
   
    def __init__(self):
        super().__init__()
        self.plugin_name = "MetadataExtractorPlugin"
        self.supported_formats = ["ALL"]
        self.version = "1.0.0"
        self.description = "Comprehensive metadata extraction from files"
       
    def analyze(self, file_path: str, file_data: bytes = None) -> Dict[str, Any]:
        """Extract metadata from file"""
        if file_data is None:
            with open(file_path, 'rb') as f:
                file_data = f.read(32768)  # Read 32KB for metadata analysis
       
        results = {
            'file_path': file_path,
            'analysis_method': 'metadata_extraction',
            'basic_metadata': {},
            'format_specific_metadata': {},
            'exif_data': {},
            'creation_info': {},
            'technical_metadata': {},
            'extracted_strings': [],
            'metadata_quality': {}
        }
       
        # Extract various types of metadata
        results.update({
            'basic_metadata': self._extract_basic_metadata(file_path, file_data),
            'format_specific_metadata': self._extract_format_metadata(file_data),
            'exif_data': self._extract_exif_metadata(file_data),
            'creation_info': self._extract_creation_info(file_data),
            'technical_metadata': self._extract_technical_metadata(file_data),
            'extracted_strings': self._extract_strings(file_data),
            'metadata_quality': self._assess_metadata_quality(results)
        })
       
        return results
   
    def _extract_basic_metadata(self, file_path: str, file_data: bytes) -> Dict[str, Any]:
        """Extract basic file metadata"""
        file_stats = Path(file_path)
       
        return {
            'file_name': file_stats.name,
            'file_extension': file_stats.suffix.lower(),
            'file_size': len(file_data),
            'file_size_human': self._format_file_size(len(file_data)),
            'directory': str(file_stats.parent),
            'magic_number': file_data[:8].hex() if len(file_data) >= 8 else '',
            'file_type_guess': self._guess_file_type(file_data),
            'has_standard_header': self._has_standard_header(file_data)
        }
   
    def _extract_format_metadata(self, file_data: bytes) -> Dict[str, Any]:
        """Extract format-specific metadata"""
        metadata = {}
       
        # Try different format parsers
        metadata.update(self._extract_image_metadata(file_data))
        metadata.update(self._extract_archive_metadata(file_data))
        metadata.update(self._extract_document_metadata(file_data))
        metadata.update(self._extract_audio_metadata(file_data))
        metadata.update(self._extract_executable_metadata(file_data))
       
        return metadata
   
    def _extract_exif_metadata(self, file_data: bytes) -> Dict[str, Any]:
        """Extract EXIF and similar metadata"""
        exif_data = {}
       
        # Basic EXIF-like extraction (simplified)
        if file_data.startswith(b'\xFF\xD8'):  # JPEG
            exif_data.update(self._extract_jpeg_exif(file_data))
        elif file_data.startswith(b'\x89PNG'):
            exif_data.update(self._extract_png_metadata(file_data))
        elif file_data.startswith(b'RIFF') and file_data[8:12] == b'WAVE':
            exif_data.update(self._extract_wave_metadata(file_data))
       
        return exif_data
   
    def _extract_creation_info(self, file_data: bytes) -> Dict[str, Any]:
        """Extract creation and modification information"""
        info = {
            'software_mentions': [],
            'creation_tools': [],
            'timestamps_found': [],
            'author_info': [],
            'company_references': []
        }
       
        # Extract strings that might indicate creation info
        text_content = file_data.decode('utf-8', errors='ignore').lower()
       
        # Common software signatures
        software_keywords = [
            'photoshop', 'adobe', 'microsoft', 'word', 'excel', 'powerpoint',
            'gimp', 'blender', 'maya', '3ds', 'autocad', 'libtiff',
            'created by', 'generated by', 'produced by'
        ]
       
        for keyword in software_keywords:
            if keyword in text_content:
                info['software_mentions'].append(keyword)
       
        # Look for timestamp patterns
        import re
        timestamp_patterns = [
            r'\d{4}:\d{2}:\d{2} \d{2}:\d{2}:\d{2}',  # EXIF style
            r'\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}',  # ISO 8601
            r'\d{2}/\d{2}/\d{4} \d{2}:\d{2}:\d{2}'   # US format
        ]
       
        for pattern in timestamp_patterns:
            matches = re.findall(pattern, text_content)
            info['timestamps_found'].extend(matches)
       
        return info
   
    def _extract_technical_metadata(self, file_data: bytes) -> Dict[str, Any]:
        """Extract technical metadata"""
        technical = {
            'endianness': self._detect_endianness(file_data),
            'character_encoding': self._detect_encoding(file_data),
            'compression_hints': self._detect_compression(file_data),
            'encryption_indicators': self._detect_encryption(file_data),
            'checksums_found': self._find_checksums(file_data),
            'embedded_resources': self._find_embedded_resources(file_data)
        }
       
        return technical
   
    def _extract_strings(self, file_data: bytes, min_length: int = 4) -> List[Dict[str, Any]]:
        """Extract readable strings from binary data"""
        strings = []
        current_string = []
       
        for byte in file_data:
            if 32 <= byte <= 126:  # Printable ASCII
                current_string.append(chr(byte))
            else:
                if len(current_string) >= min_length:
                    string = ''.join(current_string)
                    strings.append({
                        'string': string,
                        'length': len(string),
                        'offset': 0,  # Would need tracking in real implementation
                        'category': self._categorize_string(string)
                    })
                current_string = []
       
        # Add any remaining string
        if len(current_string) >= min_length:
            string = ''.join(current_string)
            strings.append({
                'string': string,
                'length': len(string),
                'offset': 0,
                'category': self._categorize_string(string)
            })
       
        return sorted(strings, key=lambda x: x['length'], reverse=True)[:50]  # Top 50 by length
   
    def _assess_metadata_quality(self, metadata_results: Dict[str, Any]) -> Dict[str, Any]:
        """Assess the quality and completeness of extracted metadata"""
        quality = {
            'completeness_score': 0,
            'richness_score': 0,
            'consistency_score': 0,
            'trustworthiness': 'Unknown'
        }
       
        basic_meta = metadata_results.get('basic_metadata', {})
        format_meta = metadata_results.get('format_specific_metadata', {})
        exif_data = metadata_results.get('exif_data', {})
       
        # Calculate completeness (how much metadata was found)
        total_fields = 0
        filled_fields = 0
       
        for category in [basic_meta, format_meta, exif_data]:
            for key, value in category.items():
                total_fields += 1
                if value and value != [] and value != {}:
                    filled_fields += 1
       
        if total_fields > 0:
            quality['completeness_score'] = (filled_fields / total_fields) * 100
       
        # Calculate richness (variety of metadata types)
        metadata_categories = [
            basic_meta, format_meta, exif_data,
            metadata_results.get('creation_info', {}),
            metadata_results.get('technical_metadata', {})
        ]
       
        non_empty_categories = sum(1 for category in metadata_categories if category)
        quality['richness_score'] = (non_empty_categories / len(metadata_categories)) * 100
       
        # Assess trustworthiness
        if quality['completeness_score'] > 80 and quality['richness_score'] > 60:
            quality['trustworthiness'] = 'High'
        elif quality['completeness_score'] > 50:
            quality['trustworthiness'] = 'Medium'
        else:
            quality['trustworthiness'] = 'Low'
       
        return quality
   
    def _extract_image_metadata(self, file_data: bytes) -> Dict[str, Any]:
        """Extract image-specific metadata"""
        metadata = {}
       
        if file_data.startswith(b'\xFF\xD8'):  # JPEG
            metadata.update(self._parse_jpeg_metadata(file_data))
        elif file_data.startswith(b'\x89PNG'):
            metadata.update(self._parse_png_metadata(file_data))
        elif file_data.startswith(b'GIF8'):
            metadata.update(self._parse_gif_metadata(file_data))
        elif file_data.startswith(b'BM'):
            metadata.update(self._parse_bmp_metadata(file_data))
        elif file_data.startswith(b'\x49\x49\x2A\x00') or file_data.startswith(b'\x4D\x4D\x00\x2A'):  # TIFF
            metadata.update(self._parse_tiff_metadata(file_data))
       
        return {'image_metadata': metadata} if metadata else {}
   
    def _extract_archive_metadata(self, file_data: bytes) -> Dict[str, Any]:
        """Extract archive-specific metadata"""
        metadata = {}
       
        if file_data.startswith(b'PK'):  # ZIP
            metadata.update(self._parse_zip_metadata(file_data))
        elif file_data.startswith(b'Rar!'):  # RAR
            metadata.update(self._parse_rar_metadata(file_data))
        elif file_data.startswith(b'7z'):  # 7-Zip
            metadata.update(self._parse_7z_metadata(file_data))
       
        return {'archive_metadata': metadata} if metadata else {}
   
    def _extract_document_metadata(self, file_data: bytes) -> Dict[str, Any]:
        """Extract document-specific metadata"""
        metadata = {}
       
        if file_data.startswith(b'%PDF'):  # PDF
            metadata.update(self._parse_pdf_metadata(file_data))
        elif file_data.startswith(b'PK') and b'word/' in file_data[:1000]:  # DOCX
            metadata.update(self._parse_docx_metadata(file_data))
        elif file_data.startswith(b'\xD0\xCF\x11\xE0'):  # DOC
            metadata.update(self._parse_doc_metadata(file_data))
       
        return {'document_metadata': metadata} if metadata else {}
   
    def _extract_audio_metadata(self, file_data: bytes) -> Dict[str, Any]:
        """Extract audio-specific metadata"""
        metadata = {}
       
        if file_data.startswith(b'ID3'):  # MP3 with ID3
            metadata.update(self._parse_mp3_metadata(file_data))
        elif file_data.startswith(b'RIFF') and file_data[8:12] == b'WAVE':  # WAV
            metadata.update(self._parse_wav_metadata(file_data))
        elif file_data.startswith(b'OggS'):  # OGG
            metadata.update(self._parse_ogg_metadata(file_data))
       
        return {'audio_metadata': metadata} if metadata else {}
   
    def _extract_executable_metadata(self, file_data: bytes) -> Dict[str, Any]:
        """Extract executable-specific metadata"""
        metadata = {}
       
        if file_data.startswith(b'MZ'):  # Windows PE
            metadata.update(self._parse_pe_metadata(file_data))
        elif file_data.startswith(b'\x7FELF'):  # ELF
            metadata.update(self._parse_elf_metadata(file_data))
       
        return {'executable_metadata': metadata} if metadata else {}
   
    def _parse_jpeg_metadata(self, file_data: bytes) -> Dict[str, Any]:
        """Parse JPEG metadata"""
        metadata = {'format': 'JPEG'}
       
        try:
            # Look for SOF (Start of Frame) marker
            sof_markers = [
                (b'\xFF\xC0', 'SOF0'), (b'\xFF\xC1', 'SOF1'), (b'\xFF\xC2', 'SOF2')
            ]
           
            for marker, name in sof_markers:
                pos = file_data.find(marker)
                if pos != -1 and pos + 8 < len(file_data):
                    # Extract basic image info from SOF
                    length = struct.unpack('>H', file_data[pos+2:pos+4])[0]
                    precision = file_data[pos+4]
                    height = struct.unpack('>H', file_data[pos+5:pos+7])[0]
                    width = struct.unpack('>H', file_data[pos+7:pos+9])[0]
                    components = file_data[pos+9]
                   
                    metadata.update({
                        'width': width,
                        'height': height,
                        'color_components': components,
                        'precision_bits': precision,
                        'sof_marker': name
                    })
                    break
           
            # Look for APP0 (JFIF) or APP1 (EXIF) markers
            if file_data.find(b'JFIF') != -1:
                metadata['jfif_present'] = True
            if file_data.find(b'Exif') != -1:
                metadata['exif_present'] = True
               
        except Exception as e:
            metadata['error'] = f'JPEG parsing failed: {str(e)}'
       
        return metadata
   
    def _parse_png_metadata(self, file_data: bytes) -> Dict[str, Any]:
        """Parse PNG metadata"""
        metadata = {'format': 'PNG'}
       
        try:
            # PNG header is at offset 8, IHDR chunk at offset 12
            if len(file_data) >= 24:
                width = struct.unpack('>I', file_data[16:20])[0]
                height = struct.unpack('>I', file_data[20:24])[0]
                bit_depth = file_data[24]
                color_type = file_data[25]
                compression = file_data[26]
                filter_method = file_data[27]
                interlace = file_data[28]
               
                metadata.update({
                    'width': width,
                    'height': height,
                    'bit_depth': bit_depth,
                    'color_type': color_type,
                    'compression_method': compression,
                    'filter_method': filter_method,
                    'interlace_method': interlace
                })
               
                # Look for text chunks
                if b'tEXt' in file_data or b'iTXt' in file_data or b'zTXt' in file_data:
                    metadata['text_chunks_present'] = True
                   
        except Exception as e:
            metadata['error'] = f'PNG parsing failed: {str(e)}'
       
        return metadata
   
    def _parse_gif_metadata(self, file_data: bytes) -> Dict[str, Any]:
        """Parse GIF metadata"""
        metadata = {'format': 'GIF'}
       
        try:
            if len(file_data) >= 13:
                version = file_data[3:6].decode('ascii', errors='ignore')
                width = struct.unpack('<H', file_data[6:8])[0]
                height = struct.unpack('<H', file_data[8:10])[0]
               
                packed_field = file_data[10]
                has_gct = (packed_field & 0x80) != 0
                color_resolution = ((packed_field & 0x70) >> 4) + 1
                gct_size = 2 << (packed_field & 0x07)
                bg_color_index = file_data[11]
                aspect_ratio = file_data[12]
               
                metadata.update({
                    'version': version,
                    'width': width,
                    'height': height,
                    'has_global_color_table': has_gct,
                    'color_resolution': color_resolution,
                    'global_color_table_size': gct_size,
                    'background_color_index': bg_color_index,
                    'pixel_aspect_ratio': aspect_ratio
                })
               
        except Exception as e:
            metadata['error'] = f'GIF parsing failed: {str(e)}'
       
        return metadata
   
    def _parse_bmp_metadata(self, file_data: bytes) -> Dict[str, Any]:
        """Parse BMP metadata"""
        metadata = {'format': 'BMP'}
       
        try:
            if len(file_data) >= 26:
                # BITMAPFILEHEADER
                file_size = struct.unpack('<I', file_data[2:6])[0]
                data_offset = struct.unpack('<I', file_data[10:14])[0]
               
                # BITMAPINFOHEADER (assuming version 3)
                header_size = struct.unpack('<I', file_data[14:18])[0]
                width = struct.unpack('<I', file_data[18:22])[0]
                height = struct.unpack('<I', file_data[22:26])[0]
                planes = struct.unpack('<H', file_data[26:28])[0]
                bit_count = struct.unpack('<H', file_data[28:30])[0]
                compression = struct.unpack('<I', file_data[30:34])[0]
                image_size = struct.unpack('<I', file_data[34:38])[0]
               
                metadata.update({
                    'file_size': file_size,
                    'data_offset': data_offset,
                    'header_size': header_size,
                    'width': width,
                    'height': height,
                    'planes': planes,
                    'bits_per_pixel': bit_count,
                    'compression': compression,
                    'image_size': image_size
                })
               
        except Exception as e:
            metadata['error'] = f'BMP parsing failed: {str(e)}'
       
        return metadata
   
    def _parse_zip_metadata(self, file_data: bytes) -> Dict[str, Any]:
        """Parse ZIP archive metadata"""
        metadata = {'format': 'ZIP'}
       
        try:
            # Find end of central directory
            eocd_pos = file_data.rfind(b'PK\x05\x06')
            if eocd_pos != -1:
                eocd = file_data[eocd_pos:eocd_pos+22]
                disk_number = struct.unpack('<H', eocd[4:6])[0]
                central_dir_disk = struct.unpack('<H', eocd[6:8])[0]
                disk_entries = struct.unpack('<H', eocd[8:10])[0]
                total_entries = struct.unpack('<H', eocd[10:12])[0]
                central_dir_size = struct.unpack('<I', eocd[12:16])[0]
                central_dir_offset = struct.unpack('<I', eocd[16:20])[0]
                comment_length = struct.unpack('<H', eocd[20:22])[0]
               
                metadata.update({
                    'disk_number': disk_number,
                    'central_dir_disk': central_dir_disk,
                    'disk_entries': disk_entries,
                    'total_entries': total_entries,
                    'central_dir_size': central_dir_size,
                    'central_dir_offset': central_dir_offset,
                    'comment_length': comment_length,
                    'is_multi_disk': disk_number > 0 or central_dir_disk > 0,
                    'has_comment': comment_length > 0
                })
               
        except Exception as e:
            metadata['error'] = f'ZIP parsing failed: {str(e)}'
       
        return metadata
   
    # Additional format parsers would follow similar patterns...
   
    def _extract_jpeg_exif(self, file_data: bytes) -> Dict[str, Any]:
        """Extract JPEG EXIF metadata (simplified)"""
        exif = {}
       
        # Look for APP1 marker (EXIF)
        app1_pos = file_data.find(b'\xFF\xE1')
        if app1_pos != -1:
            exif['has_exif'] = True
           
            # Extract some basic EXIF fields if possible
            if b'DateTime' in file_data[app1_pos:app1_pos+100]:
                exif['has_datetime'] = True
            if b'Make' in file_data[app1_pos:app1_pos+100]:
                exif['has_camera_make'] = True
            if b'Model' in file_data[app1_pos:app1_pos+100]:
                exif['has_camera_model'] = True
       
        return exif
   
    def _detect_endianness(self, file_data: bytes) -> str:
        """Detect file endianness"""
        if len(file_data) < 4:
            return 'Unknown'
       
        # Check for common endianness indicators
        if file_data.startswith(b'\xFE\xFF'):  # UTF-16 BE BOM
            return 'Big-endian'
        elif file_data.startswith(b'\xFF\xFE'):  # UTF-16 LE BOM
            return 'Little-endian'
        elif file_data.startswith(b'\x00\x00\xFE\xFF'):  # UTF-32 BE BOM
            return 'Big-endian'
        elif file_data.startswith(b'\xFF\xFE\x00\x00'):  # UTF-32 LE BOM
            return 'Little-endian'
       
        # Try to detect from common patterns
        try:
            # Check if first few bytes look like common little-endian values
            le_test = struct.unpack('<I', file_data[:4])[0]
            be_test = struct.unpack('>I', file_data[:4])[0]
           
            # Common header values are often small in LE
            if le_test < 0x1000 and be_test > 0x100000:
                return 'Little-endian'
            elif be_test < 0x1000 and le_test > 0x100000:
                return 'Big-endian'
        except:
            pass
       
        return 'Unknown'
   
    def _detect_encoding(self, file_data: bytes) -> List[str]:
        """Detect character encoding"""
        encodings = []
       
        # Check BOMs
        if file_data.startswith(b'\xEF\xBB\xBF'):
            encodings.append('UTF-8')
        elif file_data.startswith(b'\xFF\xFE'):
            encodings.append('UTF-16 LE')
        elif file_data.startswith(b'\xFE\xFF'):
            encodings.append('UTF-16 BE')
       
        # Statistical detection for text files
        try:
            # Try UTF-8
            file_data.decode('utf-8')
            encodings.append('UTF-8')
        except:
            pass
       
        try:
            # Try Latin-1
            file_data.decode('latin-1')
            encodings.append('ISO-8859-1')
        except:
            pass
       
        return list(set(encodings))  # Remove duplicates
   
    def _detect_compression(self, file_data: bytes) -> List[str]:
        """Detect compression methods"""
        compression = []
       
        # Check for common compression signatures
        if file_data.startswith(b'\x1F\x8B'):
            compression.append('GZIP')
        if file_data.startswith(b'BZh'):
            compression.append('BZIP2')
        if file_data.startswith(b'\xFD7zXZ'):
            compression.append('XZ')
        if file_data.startswith(b'\x28\xB5\x2F\xFD'):
            compression.append('Zstandard')
       
        # Check for compressed data patterns (high entropy)
        entropy = self._calculate_entropy(file_data[:4096])
        if entropy > 7.5:
            compression.append('Likely compressed')
       
        return compression
   
    def _detect_encryption(self, file_data: bytes) -> List[str]:
        """Detect encryption indicators"""
        indicators = []
       
        entropy = self._calculate_entropy(file_data[:4096])
        if entropy > 7.8:
            indicators.append('High entropy (possible encryption)')
       
        # Check for known encryption headers
        if file_data.startswith(b'Salted__'):
            indicators.append('OpenSSL encryption')
       
        return indicators
   
    def _find_checksums(self, file_data: bytes) -> List[Dict[str, Any]]:
        """Find potential checksums in file"""
        checksums = []
       
        # Look for common checksum patterns
        patterns = [
            (b'\x00\x00\x00\x00', 'Possible null checksum'),
            (b'\xFF\xFF\xFF\xFF', 'Possible ones checksum'),
        ]
       
        for pattern, description in patterns:
            if file_data.endswith(pattern):
                checksums.append({
                    'type': description,
                    'position': 'end_of_file',
                    'value': pattern.hex()
                })
       
        return checksums
   
    def _find_embedded_resources(self, file_data: bytes) -> List[Dict[str, Any]]:
        """Find embedded resources"""
        resources = []
       
        # Look for common embedded resource signatures
        embedded_patterns = [
            (b'PNG', 'Embedded PNG'),
            (b'JFIF', 'Embedded JPEG'),
            (b'%PDF', 'Embedded PDF'),
            (b'PK', 'Embedded ZIP'),
        ]
       
        for pattern, description in embedded_patterns:
            positions = []
            pos = file_data.find(pattern)
            while pos != -1:
                if pos > 0:  # Not at start of file
                    positions.append(pos)
                pos = file_data.find(pattern, pos + 1)
           
            if positions:
                resources.append({
                    'type': description,
                    'occurrences': len(positions),
                    'positions': positions[:5]  # Limit to first 5
                })
       
        return resources
   
    def _guess_file_type(self, file_data: bytes) -> str:
        """Guess file type from content"""
        if len(file_data) == 0:
            return "Empty"
       
        # Use Python's imghdr for images
        image_type = imghdr.what(None, h=file_data)
        if image_type:
            return f"Image ({image_type.upper()})"
       
        # Check for other common types
        if file_data.startswith(b'%PDF'):
            return "PDF Document"
        elif file_data.startswith(b'PK'):
            return "ZIP Archive"
        elif file_data.startswith(b'<!DOCTYPE html') or file_data.startswith(b'<html'):
            return "HTML Document"
        elif file_data.startswith(b'{\\rtf'):
            return "RTF Document"
       
        # Statistical guess
        printable = sum(1 for byte in file_data if 32 <= byte <= 126)
        if printable / len(file_data) > 0.9:
            return "Text File"
        elif printable / len(file_data) > 0.5:
            return "Mixed Text/Binary"
        else:
            return "Binary File"
   
    def _has_standard_header(self, file_data: bytes) -> bool:
        """Check if file has a standard header"""
        standard_headers = [
            b'\xFF\xD8', b'\x89PNG', b'GIF8', b'BM', b'%PDF', b'PK',
            b'RIFF', b'ID3', b'\x7FELF', b'MZ', b'\x1A\x45\xDF\xA3'
        ]
       
        return any(file_data.startswith(header) for header in standard_headers)
   
    def _format_file_size(self, size: int) -> str:
        """Format file size in human-readable format"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size < 1024.0:
                return f"{size:.1f} {unit}"
            size /= 1024.0
        return f"{size:.1f} TB"
   
    def _categorize_string(self, string: str) -> str:
        """Categorize extracted string"""
        string_lower = string.lower()
       
        if any(keyword in string_lower for keyword in ['http://', 'https://', 'www.', '.com', '.org']):
            return 'URL'
        elif any(keyword in string_lower for keyword in ['@', 'mail', 'email']):
            return 'Email'
        elif any(keyword in string_lower for keyword in ['password', 'pwd', 'secret', 'key']):
            return 'Security'
        elif any(keyword in string_lower for keyword in ['error', 'warning', 'debug', 'log']):
            return 'Log/Error'
        elif any(keyword in string_lower for keyword in ['function', 'class', 'def ', 'var ', 'import']):
            return 'Code'
        elif any(keyword in string_lower for keyword in ['version', 'v1.', 'v2.', 'release']):
            return 'Version'
        else:
            return 'Generic'
   
    def _calculate_entropy(self, data: bytes) -> float:
        """Calculate Shannon entropy"""
        if len(data) == 0:
            return 0.0
       
        entropy = 0.0
        for x in range(256):
            p_x = data.count(x) / len(data)
            if p_x > 0:
                entropy += -p_x * math.log2(p_x)
       
        return entropy